{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1  Importing Data Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Pandas library as  the dependency\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2 Loading a File into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'Resources/Data/state_to_state_migrations_table_2010 .csv' does not exist: b'Resources/Data/state_to_state_migrations_table_2010 .csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3eb6b104838a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import our input dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmigration_2010_load_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Resources/Data/state_to_state_migrations_table_2010 .csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmigration_2010_load_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'Resources/Data/state_to_state_migrations_table_2010 .csv' does not exist: b'Resources/Data/state_to_state_migrations_table_2010 .csv'"
     ]
    }
   ],
   "source": [
    "# Import our input dataset\n",
    "\n",
    "migration_2010_load_df = pd.read_csv('Resources/Data/state_to_state_migrations_table_2010 .csv')\n",
    "migration_2010_load_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing NaN values in dataset\n",
    "    \n",
    "migration_2010_load_df=migration_2010_load_df.replace(\"NaN\",0)\n",
    "migration_2010_load_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Delete multiple columns from the DF\n",
    "\n",
    "clean_2010Data_load_df = migration_2010_load_df.drop([ \"MOE\",\"MOE.1\", \"MOE.2\",\"MOE.3\",\"MOE.4\",\"MOE.5\",\"MOE.6\",\"MOE.7\",\"MOE.8\",\"MOE.9\",\"MOE.10\",\n",
    "                                                      \"MOE.11\",\"MOE.12\",\"MOE.13\",\"MOE.14\",\"MOE.15\",\"MOE.16\",\"MOE.17\",\"MOE.18\",\"MOE.19\",\"MOE.20\",\n",
    "                                                      \"MOE.21\",\"MOE.22\",\"MOE.23\",\"MOE.24\",\"MOE.25\",\"MOE.26\",\"MOE.27\",\"MOE.28\",\"MOE.29\",\"MOE.30\",\n",
    "                                                      \"MOE.31\",\"MOE.32\",\"MOE.33\",\"MOE.34\",\"MOE.35\",\"MOE.36\",\"MOE.37\",\"MOE.38\",\"MOE.39\",\"MOE.40\",\n",
    "                                                      \"MOE.41\",\"MOE.42\",\"MOE.43\",\"MOE.44\",\"MOE.45\",\"MOE.46\",\"MOE.47\",\"MOE.48\",\"MOE.49\",\"MOE.50\",\n",
    "                                                      \"MOE.51\",\"MOE.52\",\"MOE.53\",\"MOE.54\"], axis=1)\n",
    "clean_2010Data_load_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename a column\n",
    "clean_2010Data_load_df.rename(columns = {'Current residence in --': 'Current residence'},inplace = True) \n",
    "clean_2010Data_load_df.head()\n",
    "\n",
    "# Move column up the col list\n",
    "region = clean_2010Data_load_df['Region']\n",
    "clean_2010Data_load_df.drop(labels=['Region'], axis=1,inplace = True)\n",
    "clean_2010Data_load_df.insert(0, 'Region', region)\n",
    "clean_2010Data_load_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove all the ',' from the entire DF at once\n",
    "clean_2010Data_load_df.replace(',','', regex=True, inplace=True)\n",
    "clean_2010Data_load_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop Non-numeric columns,so to be able to change data types from object-float\n",
    "Drop_2010Data_load_df = clean_2010Data_load_df .drop(['Region', 'Current residence'], axis=1)\n",
    "Drop_2010Data_load_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for data types\n",
    "Drop_2010Data_load_df.dtypes\n",
    "\n",
    "# Converting all columns to numeric datatype \n",
    "Drop_2010Data_load_df.astype('float').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the State column\n",
    "Drop_2010Data_load_df.insert(0, 'Region', clean_2010Data_load_df['Region'])\n",
    "Drop_2010Data_load_df.insert(0, 'Current residence', clean_2010Data_load_df['Current residence'])\n",
    "Drop_2010Data_load_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving cleaned data\n",
    "file_path = \"C:/Users/aodom/OneDrive/Documents/Class_Folder_Berkeley/Migration_Flows_ADominguez/CA_Migration_Flow_Analysis/Data/2010_migration_flow.csv\"\n",
    "Drop_2010Data_load_df.to_csv(file_path, index=False)\n",
    "\n",
    "    #OR\n",
    "# migration_2010_load_df.to_csv(r'C:/Users/aodom/OneDrive/Documents/Class_Folder_Berkeley/Migration_Flows_ADominguez/CA_Migration_Flow_Analysis/Data/migration_flow.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Region\n",
    "Drop_2010Data_load_df.groupby(['Region']).groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics \n",
    "# df['column'].describe()\n",
    "\n",
    "# Group data by region and summarize \"Different state of residence (1yr ago) Total Est\"\n",
    "Drop_2010Data_load_df.groupby([\"Region\"])[[\"Different state of residence (1yr ago) Total Est\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of data points from the Region.\n",
    "\n",
    "sum(Drop_2010Data_load_df[\"Region\"]==\"Midwest_states\")\n",
    "sum(Drop_2010Data_load_df[\"Region\"]==\"Northeast_states\")\n",
    "sum(Drop_2010Data_load_df[\"Region\"]==\"South_states\")\n",
    "sum(Drop_2010Data_load_df[\"Region\"]==\"West_states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index to Region, then determine the state type.\n",
    "per_region = Drop_2010Data_load_df.set_index([\"Region\"])[\"Current residence\"]\n",
    "per_region.head()\n",
    "\n",
    "# Convert series to DF\n",
    "df = pd.DataFrame(per_region)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2)  Calculate the total num states in a region\n",
    "per_region_count = Drop_2010Data_load_df[\"Region\"].value_counts()\n",
    "per_region_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Migration \"Out\"-flow per State\n",
    "per_school_counts = Drop_2010Data_load_df.set_index([\"Current residence\"])[\"Different state of residence (1yr ago) Total Est\"]\n",
    "per_school_counts.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum migration per region.\n",
    "# Using the groupby() method, instead of the set_index method will prevent the return of every occurence of the school name.\n",
    "\n",
    "per_region = Drop_2010Data_load_df.groupby([\"Region\"])[[\"Different state of residence (1yr ago) Total Est\"]].mean()\n",
    "per_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many CA migrations per Region\n",
    "per_region_from_CA = Drop_2010Data_load_df.set_index([\"Region\"])[\"California\"]\n",
    "per_region_from_CA= Drop_2010Data_load_df.groupby([\"Region\"])[[\"California\"]].sum()\n",
    "\n",
    "per_region_from_CA\n",
    "\n",
    "\n",
    "# Convert series to DF\n",
    "df1 = pd.DataFrame(per_region_from_CA)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column using dictionary \n",
    "new_data = { \"West_states\": ['Arizona', 'Colorado', 'Idaho', 'Montana', 'Nevada', 'New Mexico', 'Utah', 'Wyoming', 'Alaska', 'California', 'Hawaii', 'Oregon', 'Washington'],\n",
    "    \"Midwest_states\":['Illinois','Indiana', 'Michigan', 'Ohio', 'Wisconsin', 'Iowa', 'Kansas', 'Minnesota', 'Missouri', 'Nebraska', 'North Dakota', 'South Dakota'],\n",
    "    \"Northeast_states\":['Connecticut', 'Maine', 'Massachusetts', 'New Hampshire', 'Rhode Island', 'Vermont', 'New Jersey', 'New York', 'Pennsylvania'],\n",
    "    \"South_states\":['Delaware', 'Florida', 'Georgia', 'Maryland', 'North Carolina', 'South Carolina', 'Virginia', 'District of Columbia', 'West Virginia', 'Alabama', 'Kentucky', \n",
    "                    'Missisipi', 'Tennessee', 'Arkansas', 'Louisiana', 'Oklahoma', 'Texas']\n",
    "};\n",
    "new_data\n",
    "\n",
    "#migration_2010_load_df.groupby(['Region', 'Current residence in --'])['West_states'].sum()\n",
    "Drop_2010Data_load_df.groupby('Region', as_index=False).agg({\"Different state of residence (1yr ago) Total Est\": \"sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sum migration flow from CA to Each Region\n",
    "sumCA = Drop_2010Data_load_df.groupby([\"Region\"]).sum()[\"California\"]\n",
    "sumCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate states with high migration flows by creating a filtered DataFrame.\n",
    "highest_mig2010 = Drop_2010Data_load_df[(Drop_2010Data_load_df[\"Different state of residence (1yr ago) Total Est\"] >= 100000)]\n",
    "highest_mig2010 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DF for Each Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New DF can be created by filtering the DF, where the Region is specified.\n",
    "# Create the West_states DataFrame.\n",
    "\n",
    "West_states_df = Drop_2010Data_load_df[Drop_2010Data_load_df[\"Region\"] == \"West_states\"]\n",
    "West_states_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Midwest_cities DataFrame.\n",
    "\n",
    "Midwest_states_df = Drop_2010Data_load_df[Drop_2010Data_load_df[\"Region\"] == \"Midwest_states\"]\n",
    "Midwest_states_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Northeast_states DataFrame.\n",
    "Northeast_states_df = Drop_2010Data_load_df[Drop_2010Data_load_df[\"Region\"] == \"Northeast_states\"]\n",
    "Northeast_states_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the South States DataFrame.\n",
    "South_states_df = Drop_2010Data_load_df[Drop_2010Data_load_df[\"Region\"] == \"South_states\"]\n",
    "South_states_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gapminder_pop.groupby(\"continent\").describe()\n",
    "df.groupby('series_id')['value'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Drop_2010Data_load_df.loc['Different state of residence (1yr ago) Total Est'] = Drop_2010Data_load_df.sum()\n",
    "Drop_2010Data_load_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the groupby_dict  \n",
    "groupby_dict = {\"West_states\":\"West\", \n",
    "           \"Midwest_states\":\"Midwest\", \n",
    "           \"Northeast_states\":\"Northeast\", \n",
    "           \"South_states\":\"South\",\n",
    "           \"Current residence in --\":\"Current Residence\", \n",
    "           \"Population (1yr ago) Est\":\"Population (1yr ago) Est\",\n",
    "           \"MOE\":\"MOE\", \n",
    "           \"Same house (1yr ago) Est --\":\"Same house (1yr ago) Est\", \n",
    "           \"Same state of residence (1yr ago) Est\":\"Same state of residence (1yr ago) Est\", \n",
    "           \"Different state of residence (1yr ago) Total Est\":\"Different state of residence (1yr ago) Total Est\", } \n",
    "  \n",
    "# df = df.set_index('Current residence in --') \n",
    "cleaned_2010Data_df= migration_2010_load_df.groupby(groupby_dict, axis = 1).sum() \n",
    "print(cleaned_2010Data_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# migration_2010_load_df.groupby('Region').aggregate(['min', np.median, max])\n",
    "# migration_2010_load_df\n",
    "\n",
    "df2 = migration_2010_load_df.set_index('Current residence in --')\n",
    "mapping = {'California': 'West_states', 'Tennesse': 'South_states', 'Rhode Island': 'Northeast_states', 'Minnesota': 'Midwest_states'}\n",
    "display('df2', 'df2.groupby(mapping).sum()')\n",
    "df2.groupby(mapping).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migration_2010_load_df.loc['California'] = migration_2010_load_df.sum()\n",
    "migration_2010_load_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of Migration Flow by Region\n",
    "#migration_2010_load_df .groupby(['Current residence in --','Region']).sum()\n",
    "\n",
    "#migration_2010_load_df(['Region', 'Current residence in --'])['Alabama'].agg('sum')\n",
    "#sum(migration_2010_load_df[\"Current residence in --\": 'California']==\"West_states\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
